{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://datasciencecampus.ons.gov.uk/wp-content/uploads/sites/10/2017/03/data-science-campus-logo-new.svg\"\n",
    "             alt=\"ONS Data Science Campus Logo\"\n",
    "             width = \"240\"\n",
    "             style=\"margin: 0px 60px\"\n",
    "             />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Solutions\n",
    "\n",
    "This notebook contains all the solutions to this project, as well as example visualisations for section 3. You may produce different visualisations, but this doesn't matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries we need\n",
    "import json\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.data.path.append(\"../pre_course/nltk_data\")\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import statistics\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import gensim\n",
    "from gensim import models\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use of emoji is optional!\n",
    "# import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"../Data/tweets.tsv\",sep='\\t') # replace with your file location\n",
    "\n",
    "def replace_tweet_number(cell_contents):\n",
    "    \"\"\"\n",
    "    By default the long Tweet IDs are displayed\n",
    "    as exponentials. We convert them to strings\n",
    "    as it makes it easier to read and doesn't\n",
    "    impact our processing.\n",
    "    \n",
    "    params\n",
    "    ------\n",
    "    cell_contents:  float\n",
    "                    Either a Tweet ID or NaN\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return str(int(cell_contents))\n",
    "    except ValueError:\n",
    "        return cell_contents\n",
    "\n",
    "tweets['repliedto_tweet'] = tweets['repliedto_tweet'].apply(replace_tweet_number)\n",
    "tweets['quoted_tweet'] = tweets['quoted_tweet'].apply(replace_tweet_number)\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Preparing the data for sentiment analysis\n",
    "\n",
    "The Vader package is able to process text that has been only minimally processed, and calculates sentiment based not just on words but also on punctuation, capitalisation, and emojis. For this reason we should be careful not to over-process our Tweets and instead only remove elements that will not contribute to the sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.1 Removing hyperlinks and twitter handles\n",
    "\n",
    "Hyperlinks and Twitter handles carry no meaning that would affect the sentiment, so we will remove them using regular expression (regex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the pattern we want to remove using regex\n",
    "pattern = r'(https?://[^\"\\s]+)|(@\\w+)'\n",
    "# replace the hyperlinks and twitter handles with \"\" using pandas string methods\n",
    "tweets['sentiment_analysis_text'] = tweets['text'].str.replace(pattern, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing the tweets after removing hyperlinks and twitter handles\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Tokenising the Tweets\n",
    "\n",
    "Tweets are made up of multiple sentences, which may each carry their own sentiment. We will use nltk's sentence tokeniser to split the Tweets into sentences for sentiment analysis. Later we can find the average sentiment of a Tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying nltk's built in sentence tokenizer to the tweets\n",
    "tweets['sentiment_analysis_text'] = tweets['sentiment_analysis_text'].apply(nltk.sent_tokenize)\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Removing unnecessary punctuation\n",
    "\n",
    "Vader's sentiment analysis tool can analyse punctuation including '!' and '?', but some punctuation appears in Tweets without having any meaning attached, for example, the hashtag. These can be removed without losing meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_twitter_punct(sentence_list, remove):\n",
    "    \"\"\"\n",
    "    Remove punctuation from user-specified list.\n",
    "    \n",
    "    params:\n",
    "    ------\n",
    "    ptext    (str) input text\n",
    "    remove   (str) punctuation symbols to remove\n",
    "    \n",
    "    returns\n",
    "    ------\n",
    "    Text without punctuation from list\n",
    "    \"\"\"\n",
    "    # remove the &amp symbols\n",
    "    new_sentence_list = [re.sub(string = sent, pattern = r\"&amp\", repl=\"\") for sent in sentence_list]\n",
    "    # remove selected punctuation marks\n",
    "    new_sentence_list = [re.sub(string = sent, pattern = f\"[{remove}]\", repl=\"\").strip() for sent in new_sentence_list]\n",
    "    # remove double spaces\n",
    "    new_sentence_list = [re.sub(string = sent, pattern = r\"\\s+\", repl=\" \") for sent in new_sentence_list]\n",
    "    # remove space from before punctuation mark\n",
    "    new_sentence_list = [re.sub(string = sent, pattern = r\"\\s+(?=[!?])\", repl=\"\") for sent in new_sentence_list]\n",
    "    return new_sentence_list\n",
    "\n",
    "punctuation_to_remove = r\"<>$£%&_,;:'’:#\\n.\\(\\)\\[\\]-\" # what would you define as punctuation that carries no meaning?\n",
    "\n",
    "tweets['sentiment_analysis_text'] = tweets['sentiment_analysis_text'].apply(remove_twitter_punct, remove = punctuation_to_remove)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is enough pre-processing to prepare the Tweets for sentiment analysis using Vader. We have not:\n",
    "\n",
    "* removed all punctuation, only a selection of symbols we have deemed unimportant\n",
    "* removed numbers\n",
    "* lowercased the text\n",
    "* removed stopwords\n",
    "\n",
    "We have justified these decisions by saying that Vader can handle complex text and is able to analyse sentiment using features like capitalisation and punctuation. You might want to experiment by performing more or less pre-processing than us, to see what effect it has on the sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Preparing the data for topic modelling\n",
    "\n",
    "Further pre-processing steps are required for topic modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Lowercasing\n",
    "If the text is in the same case, it is much easier for our model to interpret the words because the lower case and upper case will be treated the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase the text using pandas string methods\n",
    "tweets['processed_text'] = tweets['text'].str.lower()\n",
    "\n",
    "# remove hyperlinks and handles again\n",
    "tweets['processed_text'] = tweets['processed_text'].str.replace(pattern, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing the tweets after lowercasing text\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Extracting the meaning of emojis\n",
    "\n",
    "Depending on the task at hand, we may choose to either:\n",
    "\n",
    "* Remove emojis entirely\n",
    "* Replace the emoji with its equivalent meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use of emoji is optional!\n",
    "\n",
    "def replace_emojis(ptext):\n",
    "    \"\"\"\n",
    "    Replace any emojis in the tweets with its meaning in words\n",
    "        \n",
    "    params\n",
    "    ------\n",
    "    ptext:  str\n",
    "            Text containing emojis\n",
    "    \"\"\"\n",
    "    ptext = emoji.demojize(ptext, delimiters=(\"\", \"\"))\n",
    "    return ptext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative to using emoji\n",
    "\n",
    "def remove_emojis(ptext):\n",
    "    \"\"\"\n",
    "    Remove any UTF-8 characters (including emojis)\n",
    "    \n",
    "    params\n",
    "    ------\n",
    "    ptext:  str\n",
    "            Text from which to remove characters\n",
    "    \"\"\"\n",
    "    ptext = ptext.encode('ascii', 'ignore').decode()\n",
    "    return ptext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the functions to the data\n",
    "\n",
    "# if using emoji\n",
    "# tweets['processed_text'] = tweets['processed_text'].apply(replace_emojis)\n",
    "\n",
    "# otherwise\n",
    "tweets['processed_text'] = tweets['processed_text'].apply(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing the data after emojis have been removed\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Removing Punctuation\n",
    "\n",
    "We can remove punctuation from the corpus that is not relevant to our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the standard string punctuation\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a regular expression which captures all the above punctuation characters\n",
    "\"[{}]\".format(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that uses regex to remove punctuation from strings\n",
    "def remove_punct(ptext):\n",
    "    \"\"\"\n",
    "    replace any punctuation with nothing \"\", effectively removing it\n",
    "    \n",
    "    params\n",
    "    ------\n",
    "    ptext:  str\n",
    "            Text from which to remove punctuation\n",
    "    \"\"\"\n",
    "    ptext = re.sub(string=ptext,\n",
    "                   pattern=f\"[{string.punctuation}]\",\n",
    "                   repl=\"\")\n",
    "    return ptext\n",
    "\n",
    "# by making a function that works for one piece of text\n",
    "# we can then apply the function to all the pandas text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-use our previous function to remove &amps and newlines\n",
    "tweets['processed_text'] = tweets['processed_text'].apply(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing data after punctuation has been removed\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Tokenizing the data\n",
    "\n",
    "As explained above, we can use nltk's sentence tokeniser to split the data into sentences for sentiment analysis. Tokens don't have to be sentences, they can also be words or other sized pieces of text. Many natural language processing tasks require access to each word in a string, and we can achieve this via the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying nltk's built in word tokenizer to the tweets\n",
    "tweets['processed_text'] = tweets['processed_text'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing tokenized tweets\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Lemmatization\n",
    "\n",
    "Lemmatization involves stemming the word but makes sure that it does not lose its meaning. Lemmatization has a pre-defined dictionary that stores the context of words and checks the word in the dictionary while diminishing. In our example, lemmatization will work better than stemming, i.e \"vaccinated\" and \"vaccinates\" will change the word to \"vaccine\" rather than \"vaccin\", which is a lot more valuable to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatise(ptokens):\n",
    "    \"\"\"\n",
    "    Apply lemmatization to given list of word tokens\n",
    "    \n",
    "    params\n",
    "    ------\n",
    "    ptokens:    List[str]\n",
    "                List of word tokens\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in ptokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying lemmatization to the data\n",
    "tweets['processed_text'] = tweets['processed_text'].apply(lemmatise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing data after text has been lemmatized\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 Stopwords\n",
    "\n",
    "Stopwords are the commonly used words and are removed from the text as they do not add any value to the analysis. These words carry less or no meaning.\n",
    "\n",
    "The NLTK library consists of a list of words that are considered stopwords for the English language. We can see these below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the basic stopwords given by nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Remove stopwords from list of tokens\n",
    "    \n",
    "    params\n",
    "    ------\n",
    "    ptokens:    List[str]\n",
    "                List of word tokens\n",
    "    \"\"\"\n",
    "    # define stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    newStopWords = ['office','national','statistics','statistic','amp','ons']\n",
    "    stopwords.extend(newStopWords)\n",
    "    # loop through each token and if the word isn't in the set \n",
    "    # of stopwords keep it\n",
    "    return [item for item in tokens if item not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remving stopwords from the tweets\n",
    "tweets['processed_text'] = tweets['processed_text'].apply(clean_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing data after stopwords have been removed\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.7 Removing numbers\n",
    "\n",
    "Similar to punctuation, we can remove numbers from text data as they may hold no relevance to the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(ptokens):\n",
    "    \"\"\"\n",
    "    Keeps only alphabetic text\n",
    "    \n",
    "    params\n",
    "    ------\n",
    "    ptokens:    List[str]\n",
    "                List of word tokens\n",
    "    \"\"\"\n",
    "    return [token for token in ptokens if token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the above function to the data\n",
    "tweets['processed_text'] = tweets['processed_text'].apply(remove_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing tweets after only alphabetic text remains\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.8 Removing words less than length 2\n",
    "\n",
    "Ideally, we want as little noise as possible present in our text data to make sure we have the highest possible quality data. Removing tokens that are less than 2 characters long will help to get rid of this noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_tokens(ptokens):\n",
    "    \"\"\"\n",
    "    Remove tokens that are less than\n",
    "    3 characters in length.\n",
    "    \n",
    "    params\n",
    "    ------\n",
    "    ptokens:    List[str]\n",
    "                List of word tokens\n",
    "    \"\"\"\n",
    "    return [token for token in ptokens if len(token) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the below function to the data\n",
    "tweets['processed_text'] = tweets['processed_text'].apply(remove_short_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viewing tweets after short tokens have been removed\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Sentiment Analysis with Tweets\n",
    "\n",
    "If you are not sure how to use VADER for sentiment analysis, take a look at the [instructions](../instructions.html#41_Sentiment_Analysis_with_VADER).\n",
    "\n",
    "Now that we have a basic grasp on how VADER works, we can apply it to our dataframe of tweets. We will use the `sentiment_analysis_text` column, as this has only been part-preprocessed. VADER will do the rest for us.\n",
    "\n",
    "To perform sentiment analysis on our dataframe of tweets, we need to do the following:\n",
    "- Read in the dataframe of cleaned tweets\n",
    "- Add the VADER metrics to the dataframe - `pos`, `neg`, `neu`, `compound`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the VADER metrics to the dataframe - pos, neg, neu, compound\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "tweets['rating'] = tweets['sentiment_analysis_text'].apply(lambda t: [analyzer.polarity_scores(sentence) for sentence in t])\n",
    "\n",
    "print(tweets['sentiment_analysis_text'][0])\n",
    "print(tweets['rating'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_sentiment(list_of_score_dicts, key):\n",
    "    \"\"\"\n",
    "    Calculate the mean score for the given key for a list of dictionaries.\n",
    "    \n",
    "    params\n",
    "    ------\n",
    "    list_of_score_dicts:    List[Dict]\n",
    "                            A list of dictionaries containing VADER scores\n",
    "                            \n",
    "    key:                    str\n",
    "                            Key of dictionary to extract to calculate mean.\n",
    "    \"\"\"\n",
    "    return statistics.mean([score[key] for score in list_of_score_dicts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dictionary into separate columns\n",
    "for key in ['pos', 'neg', 'neu', 'compound']:\n",
    "    tweets[key] = tweets['rating'].apply(calculate_average_sentiment, key = key)\n",
    "    \n",
    "tweets.drop(columns = ['rating'], inplace = True)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using .describe() to view general stats about the data\n",
    "tweets.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our sentiment scores, we can explore some different visualisations of sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Visualisation\n",
    "\n",
    "The below visualisations will help you to answer the following questions:\n",
    "\n",
    "1. Which Tweets have the most positive and most negative sentiments.\n",
    "2. On what day of the week and time of day are the most people talking about the ONS?\n",
    "3. On which times of day and days of the week do people show the most positive sentiment when talking about ONS?\n",
    "4. Which Tweets from the @ONS account generated the most positive and negative responses?\n",
    "5. Does length of Tweet have an impact on sentiment? If so what is the link between them?\n",
    "6. What topics do people associate with the ONS and what is the average sentiment of Tweets about these topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Which tweets have the most positive and negative sentiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by creating three separate word clouds which will contain, respectively:\n",
    "\n",
    "* most common words from all tweets\n",
    "\n",
    "* most common words from tweets labelled positive\n",
    "\n",
    "* most common words from tweets labelled negative\n",
    "\n",
    "Although this won't help us answer the question at hand, it would be useful to give an insight into what words and phrases are associated with positive and negative sentiments across all of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def convert_series_to_text(series):\n",
    "    text = [contents for token in list(series) for contents in token]\n",
    "    return ' '.join(text).lower()\n",
    "\n",
    "def show_wordcloud(data, title, max_words = 100):\n",
    "    \"\"\"\n",
    "    Displays a wordcloud based on the given data\n",
    "    \n",
    "    params\n",
    "    ------\n",
    "    data:       pd.Series\n",
    "                List of Tweets from which to\n",
    "                construct wordcloud\n",
    "    title:      str\n",
    "                Title of wordcloud\n",
    "    \"\"\"\n",
    "    wordcloud = WordCloud(\n",
    "        background_color = 'white',\n",
    "        max_words = max_words,\n",
    "        max_font_size = 40, \n",
    "        scale = 3,\n",
    "        random_state = 42\n",
    "    ).generate(convert_series_to_text(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize = (10, 10))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize = 20)\n",
    "        fig.subplots_adjust(top = 2.3)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "    \n",
    "# print wordcloud\n",
    "show_wordcloud(tweets[\"processed_text\"], 'Most common words from all tweets', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create wordclouds for the positive and negative Tweets, we will need to define exactly what a positive, negative or neutral Tweet is. For this course, we will say that:\n",
    "\n",
    "- a positive Tweet is a tweet with a compound score more than or equal to 0.05\n",
    "\n",
    "- a negative Tweeet is a tweet with a compound score less than or equal to -0.05\n",
    "\n",
    "- a neutral Tweet is everything else, i.e tweets with a compound score between (but not including) -0.05 and 0.05.\n",
    "\n",
    "With this information we can split all the Tweets into positive and negative Tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_str = tweets[tweets['compound'] >= 0.05]['sentiment_analysis_text'] # filter the dataframe on the compound column\n",
    "neg_str = tweets[tweets['compound'] <= -0.05]['sentiment_analysis_text']\n",
    "\n",
    "show_wordcloud(pos_str, 'Wordcloud of positive Tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_str = tweets[tweets['compound'] <= -0.05]['sentiment_analysis_text']\n",
    "show_wordcloud(neg_str, 'Wordcloud of negative Tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to answer the question, we can filter the Tweets to include only positive ones, and order them by their compound score (desceding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show positive tweets and order by 'positivity' i.e. show tweets with highest positivity score first.\n",
    "pos_tweets = tweets[tweets[\"compound\"] >= 0.05].sort_values(\"compound\", ascending = False)[[\"text\", \"compound\"]].head(10)\n",
    "pos_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show negative tweets and order by 'negativity' i.e. show tweets with highest negativity score first.\n",
    "neg_tweets = tweets[tweets[\"compound\"] <= -0.05].sort_values(\"compound\")[[\"text\", \"compound\"]].head(10)\n",
    "neg_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most positive tweet\n",
    "pos_tweets.iloc[2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most negative tweet\n",
    "neg_tweets.iloc[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 On what day of the week and time of day are the most people talking about the ONS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be focusing on the `created_at` column. We   will need to make sure the column is in the correct format, and then split into two separate columns, which we will call `date` and `time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change created_at column to datetime format\n",
    "tweets['created_at'] = pd.to_datetime(tweets['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating date and time into separate columns\n",
    "tweets['date'] = pd.to_datetime(tweets['created_at'], errors='coerce').dt.date\n",
    "tweets['time'] = pd.to_datetime(tweets['created_at'], errors='coerce').dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a third column named `day of week`, where 0 = Monday, 1 = Tuesday etc\n",
    "tweets['day_of_week'] = tweets['created_at'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Map each number in the day_of_week column to the day of the week in word format.\n",
    "This is not really a necessary step, but is easier to go by the word rather than the number.\n",
    "\"\"\"\n",
    "\n",
    "dw_mapping={\n",
    "    0: 'Monday', \n",
    "    1: 'Tuesday', \n",
    "    2: 'Wednesday', \n",
    "    3: 'Thursday', \n",
    "    4: 'Friday',\n",
    "    5: 'Saturday', \n",
    "    6: 'Sunday'\n",
    "} \n",
    "tweets['day_of_week_name']=tweets['created_at'].dt.weekday.map(dw_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the day that ONS is mentioned most in our dataset\n",
    "a = tweets['date'].value_counts().idxmax()\n",
    "b = tweets['day_of_week_name'].value_counts().idxmax()\n",
    "\n",
    "print(f\"ONS was mentioned most frequently on {a}, which is a {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the most common day of the week for people to tweet in our dataset\n",
    "tweets['day_of_week_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create bar chart to visualise findings\n",
    "\n",
    "day = tweets['day_of_week_name']\n",
    "values = tweets['day_of_week_name'].value_counts()\n",
    "\n",
    "ax = tweets[['day_of_week_name']], values.plot(kind='bar',\n",
    "                                               title =\"On what day of the week are the most people talking about the ONS?\",\n",
    "                                               figsize=(10, 5),\n",
    "                                               ylabel='Number of tweets',\n",
    "                                               xlabel='Day of the week',\n",
    "                                               color = 'crimson',\n",
    "                                               legend=False,\n",
    "                                               fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this barchart, we can clearly see that Thursday is the day of the week where the most people are talking about ONS, while Monday is the day where the least number of people mention ONS. Can you think of any reasons why this could be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split time into hourly increments, for example any tweet published between 16:00:00 and 16:59:59 will return 16\n",
    "tweets['hour'] = tweets['created_at'].apply(lambda x: x.time().hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the most common time of day for people to tweet in our dataset\n",
    "\n",
    "times = tweets.hour.value_counts()\n",
    "times.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create bar chart to visualise findings\n",
    "\n",
    "ax = times.plot(kind='bar',\n",
    "                title =\"What time of day do most people talk about the ONS?\",\n",
    "                figsize=(10, 5),\n",
    "                xlabel='Hour of the day',\n",
    "                ylabel='Number of tweets',\n",
    "                color = 'cornflowerblue',\n",
    "                legend=False,\n",
    "                fontsize=12)\n",
    "plt.xticks(rotation=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of this barchart are what you might expect, with few people talking about ONS in the early hours of the morning. There is an sharp increase in conversation between 4am and 10am, which is the hour where most people are talking about ONS. There's a slow decrease in people talking about ONS as the day goes on, quite a dip at 1pm before increasing again at 2pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 On which times of day and days of the week do people show the most positive sentiment when talking about ONS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need to know the 'compound' score and hour of the Tweets\n",
    "tweet_times = tweets.filter(['text', 'compound', 'hour'], axis = 1)\n",
    "tweet_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the mean sentiment for each hourly increment of the day\n",
    "tweet_times = tweet_times.groupby(['hour']).mean()\n",
    "tweet_times = tweet_times.sort_values(by = 'compound', ascending=False)\n",
    "tweet_times.reset_index(inplace=True)\n",
    "\n",
    "tweet_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the bar chart above, it's not very common to see tweets late at night or very early in the morning. To make our data slightly simpler to look at, we could only include daytime tweets, for example from 7am-7pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop times that are outside of 7-19\n",
    "tweet_times_daytime = tweet_times[(tweet_times['hour']>=7) & (tweet_times['hour']<=19)]\n",
    "tweet_times_daytime.sort_values(by='hour',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have got rid of 'unsociable hours', we can see midday has the most positive sentiment, so we can suggest to the comms team that if they would like to get the most positive reaction to their tweets then this is the time they should post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create horizontal bar chart to display findings\n",
    "ax2 = tweet_times_daytime.plot(kind='bar',\n",
    "                             x='hour',\n",
    "                             ylabel='compound',\n",
    "                             title =\"What time of day do people show the most positive sentiment towards ONS?\",\n",
    "                             figsize=(10, 5),\n",
    "                             color = 'coral',\n",
    "                             legend=False,\n",
    "                             fontsize=12)\n",
    "plt.xticks(rotation=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike our bar chart for the most popular times of day for people to  be talking about ONS, there's no clear pattern here. Sentiment generally tends to be more positive in the morning, with a very big dip in compund score at 2pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need to know the 'compound' score and day of the Tweets\n",
    "tweet_days = tweets.filter(['text', 'compound', 'day_of_week_name'], axis = 1)\n",
    "tweet_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the mean sentiment for each day of the week\n",
    "tweet_days = tweet_days.groupby(['day_of_week_name']).mean()\n",
    "tweet_days = tweet_days.sort_values(by = 'compound', ascending=False)\n",
    "tweet_days.reset_index(inplace=True)\n",
    "\n",
    "tweet_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create horizontal bar chart to display findings\n",
    "ax2 = tweet_days.plot(kind='barh',\n",
    "                      x='day_of_week_name',\n",
    "                      xlabel='day of week',\n",
    "                      ylabel='Compound',\n",
    "                      title =\"What day of the week do people show the most positive sentiment towards ONS?\",\n",
    "                      figsize=(10, 5),\n",
    "                      color = 'green',\n",
    "                      legend=False,\n",
    "                      fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our charts above, we discovered that Saturdays and Sundays tend to be quite quiet in terms of people talking about ONS. From this chart, we can see that the people who are talking about ONS are being quite negative about it! In contrast to the weekend, the most popular day of the week to mention ONS was a Thursday, which is also showing up with quite a negative sentiment in this chart. Only three days out of seven have an overall positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Which tweets from the @ONS account generated the most positive and negative responses?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question, we will be focusing on tweets that are in reply to the ONS, so we will need to filter our dataframe to reflect this. We can then further filter it by showing the `repliedto_tweet` id and the positive/negative sentiment score of each tweet. Then, by sorting our new dataframe by positive and negative sentiment, we can find the tweet that generated the most positive and negative responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only show tweets that are in reply to @ONS\n",
    "ons_tweets = tweets.loc[tweets['in_reply_to_ons'] == True]\n",
    "ons_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the dataframe to only show columns we need\n",
    "ons_tweets = ons_tweets.filter(['text', 'repliedto_tweet', 'compound'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove any null values\n",
    "ons_tweets = ons_tweets.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ons_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the mean sentiment for each replied to tweet id\n",
    "ons_tweets = ons_tweets.groupby(['repliedto_tweet']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by positive sentiment\n",
    "ons_tweets.sort_values(by=['compound'], ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this information, we can copy the tweet id of the first row and add it to the following URL:\n",
    "\n",
    "`http://twitter.com/ons/status/tweet-id-here`\n",
    "\n",
    "This will then bring up the tweet that we are looking for. We can do the same for negative tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by negative sentiment\n",
    "ons_tweets.sort_values(by=['compound']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Does length of tweet have an impact on sentiment? If so, what is the link between them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_tweets(num):\n",
    "    \"\"\"\n",
    "    Label tweets as either pos, neg or neu\n",
    "    \n",
    "    params\n",
    "    ------\n",
    "    num:    float\n",
    "            Compound score from VADER sentiment\n",
    "            analysis indicating sentiment.\n",
    "    \"\"\"\n",
    "    if num >=0.05:\n",
    "        return 'pos'\n",
    "    elif num <= -0.05:\n",
    "        return 'neg'\n",
    "    else:\n",
    "        return 'neu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a column in the tweets dataframe with tweet labels\n",
    "tweets['label'] = tweets['compound'].apply(label_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_word_count(tweet):\n",
    "    \"\"\"\n",
    "    Calculates the number of words in the given Tweet.\n",
    "    \n",
    "    params\n",
    "    ------\n",
    "    tweet:      List[str]\n",
    "                Sentence-tokenized Tweet string\n",
    "    \"\"\"\n",
    "    return sum([len(sent.split(' ')) for sent in tweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the length of each value in the sentiment_analysis_text column\n",
    "tweets['length'] = tweets['sentiment_analysis_text'].apply(tweet_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new df with only the columns we need\n",
    "sentiment_length_df = tweets[['sentiment_analysis_text','length','compound','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing data into X and y values\n",
    "x = sentiment_length_df[['length']]\n",
    "y = sentiment_length_df[['compound']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show findings with scatter plot\n",
    "\n",
    "plt.scatter(x, y, alpha=0.2)\n",
    "plt.xlabel(\"length of tweet\")\n",
    "plt.ylabel(\"compound\")\n",
    "plt.title(\"Does the length of a tweet affect its sentiment?\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No correlation between predicted value of sentiment and actual value of sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 What topics do people associate with ONS, and what is the overall sentiment of tweets about these topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use **Latent Dirichlet Allocation** (LDA), one of the most popular topic modelling algorithms, to extract topics from our tweets. For this project, we'll define a topic as a collection of dominant keywords that are typical representatives.\n",
    "\n",
    "If you are unfamiliar with topic modelling, take some time to read through the topic modelling section in the [instructions](../instructions.html#41_Sentiment_Analysis_with_VADER) document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = tweets['processed_text']\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = gensim.corpora.Dictionary(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dictionary\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "bow_corpus = [dictionary.doc2bow(tweet) for tweet in processed_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through range of k-topics fitting LDA model to each and computing coherence scores for each model\n",
    "coherenceList_umass = []\n",
    "coherenceList_cv = []\n",
    "num_topics_list = np.arange(4,14+1)\n",
    "for num_topics in num_topics_list:\n",
    "    lda = models.LdaMulticore(corpus=bow_corpus, num_topics=num_topics, id2word=dictionary, \n",
    "                              passes=10,chunksize=4000,random_state=0)\n",
    "    cm = CoherenceModel(model=lda, corpus=bow_corpus, \n",
    "                        dictionary=dictionary, coherence='u_mass')\n",
    "    coherenceList_umass.append(cm.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot coherence scores across topic numbers\n",
    "\n",
    "plotData = pd.DataFrame({'Number of topics':num_topics_list,\n",
    "                         'CoherenceScore':coherenceList_umass})\n",
    "f,ax = plt.subplots(figsize=(16,10))\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(font_scale = 2)\n",
    "sns.pointplot(x='Number of topics', y= 'CoherenceScore',data=plotData)\n",
    "plt.axhline(y=-4.8, color='red')\n",
    "plt.title('Topic Coherence');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coherence score is highest at number of topics = 5, so we will use 5 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Model using BOW\n",
    "lda_model_bow = gensim.models.LdaMulticore(corpus=bow_corpus, num_topics=5, id2word=dictionary, decay=0.5,\n",
    "                                           chunksize=10000, passes=10, workers=4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_desc = []\n",
    "for idx, topic in lda_model_bow.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "lda_viz = gensimvis.prepare(lda_model_bow, bow_corpus, dictionary)\n",
    "lda_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What does our output mean? \n",
    "\n",
    "* Each bubble on the left hand side represents a topic. The bigger the bubble, the more common the topic is in our tweets. Ideally, the bubbles should be spread across all four quadrants, and shouldn't overlap too much. If you have a lot of small bubbles with a lot of overlap, then you have too many topics.\n",
    "\n",
    "* Hovering over each bubble shows the 30 most salient words related to that topic on the right hand side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index the model with the document to find out the topics and percentage contribution\n",
    "\n",
    "lda_model_bow[bow_corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case document 0 could be topic 0 (18% contribution), topic 2 (64% contribution) or topic 3 (16% contribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_topic(document, lda_model):\n",
    "    \"\"\"\n",
    "    Fetch the topics and percentage contribution of the document\n",
    "    for the given LDA model and return the topic number with the \n",
    "    highest contribution.\n",
    "    \n",
    "    params\n",
    "    -----\n",
    "    document:   List[Tuple]\n",
    "                A BOW document where each tuple represents\n",
    "                (word index, count)\n",
    "    \"\"\"\n",
    "    topics = lda_model[document]\n",
    "    topics = sorted(topics, key=lambda x:(x[1])) # sort in descending order of percent contribution\n",
    "    return topics[0][0]\n",
    "\n",
    "tweets['topic'] = [assign_topic(doc, lda_model = lda_model_bow) for doc in bow_corpus]\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {}\n",
    "\n",
    "for idx, topic in lda_model_bow.print_topics(-1):\n",
    "    topics[idx] = f\"Topic {idx}: \"\n",
    "    topic_words = []\n",
    "    for word in re.findall('[a-z]+', topic):\n",
    "        topic_words.append(word)\n",
    "    topics[idx] += ', '.join(topic_words)\n",
    "\n",
    "tweets['topic_words'] = tweets['topic'].map(topics)\n",
    "tweets[['text', 'topic', 'topic_words']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the average sentiment for each topic?\n",
    "\n",
    "tweets.groupby(['topic_words']).mean()['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.groupby(['topic']).mean()['compound'].plot(kind='bar',\n",
    "                                                  xlabel='Topic number',\n",
    "                                                  ylabel='Mean sentiment',\n",
    "                                                  title =\"What is the average sentiment of Tweets about different topics?\",\n",
    "                                                  figsize=(8, 5),\n",
    "                                                  color = 'green',\n",
    "                                                  legend=False,\n",
    "                                                    fontsize=12);\n",
    "\n",
    "for key, val in topics.items():\n",
    "    print(val)\n",
    "plt.xticks(rotation=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've successfully answered all six questions, we can get back to the ONS comms team to help them with their Twitter strategy. Here are the main points we should feed back to the team:\n",
    "\n",
    "* All of the topics we found relate to Covid-19. Topics 0, 1 and 3 are slightly positive, which could be seen as suprising as the key words include death and suicide. Only topic 4 is somewhat negative - we could tentatively conclude that overall, the topics involving ONS that people are talking about on Twitter are positive. Although topic 2 is positive, it's so close to zero we can safely say that topic 2 has a neutral sentiment.\n",
    "* However, only three days out of seven have people showing a positive sentiment towards ONS, and this is Tuesday, Wednesday and Friday. Weekend tweets should be avoided at all costs!\n",
    "* People tend to talk about the ONS late morning, and on Thursdays.\n",
    "* Midday is the time where tweets are at their most positive, but not for long; tweets between 2 and 4pm are much more negative!\n",
    "* There is no relationship between sentiment and length of tweets, so the team don't need to worry about shortening or lengthening their tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-i",
   "language": "python",
   "name": "nlp-i"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
